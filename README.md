# ğŸ§  Tiny Domain-Specific Language Model (TinyGPT)

This project builds a **small transformer-based language model** from scratch using PyTorch â€” trained on a domain-specific corpus (like biomedical or educational text). It uses:

- Custom tokenization with **Byte-Level BPE**
- A mini-GPT architecture with **TransformerEncoder layers**
- Full training and generation loop
- Nucleus (top-p) sampling for more coherent text generation

---

## ğŸ“Œ Project Goals

- Train a mini causal language model on **domain-specific text**
- Learn full ML pipeline: tokenization â†’ training â†’ generation
- Use lightweight architecture (can run on CPU or Apple Silicon GPU)

---

## ğŸ“‚ Folder Structure

```
tiny-domain-llm/
â”œâ”€â”€ data/
â”‚   â””â”€â”€ clean_corpus.txt        â† Plain text corpus used for training
â”œâ”€â”€ tokenizer/
â”‚   â”œâ”€â”€ vocab.json              â† Learned token-to-ID mapping
â”‚   â””â”€â”€ merges.txt              â† Merge rules from BPE tokenizer
â”œâ”€â”€ logs/
â”‚   â””â”€â”€ train_loss.txt          â† Training loss logged per epoch
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ train_tokenizer.py      â† Trains the ByteLevelBPE tokenizer
â”‚   â”œâ”€â”€ train_lm.py             â† Main training loop for the LM
â”‚   â””â”€â”€ generate.py             â† Uses trained model to generate text
â”œâ”€â”€ tiny_gpt_epoch10.pth       â† Model checkpoint after 10 epochs
â””â”€â”€ README.md                   â† You're here!
```

---

## ğŸ§  How It Works

### 1. **Tokenizer Training** (`scripts/train_tokenizer.py`)
- Uses HuggingFace's `ByteLevelBPETokenizer`
- Trains on your `clean_corpus.txt`
- Outputs:
  - `vocab.json`: token â†’ ID map
  - `merges.txt`: BPE merge rules

---

### 2. **Language Model Training** (`scripts/train_lm.py`)
- Loads tokenized corpus
- Defines a tiny GPT-like model using:
  - Embeddings
  - Positional encodings
  - 4 TransformerEncoder layers
- Trains via **causal language modeling** (predict next token)
- Saves model checkpoints + loss logs

---

### 3. **Text Generation** (`scripts/generate.py`)
- Loads trained model + tokenizer
- Supports:
  - Top-p (nucleus) sampling
  - Temperature control
- Generates text from any prompt

---

## ğŸ› ï¸ Setup Instructions

### âœ… 1. Clone the Repo
```bash
git clone https://github.com/<your-username>/tiny-domain-llm.git
cd tiny-domain-llm
```

### âœ… 2. Create and Activate Virtual Environment
```bash
python3 -m venv venv
source venv/bin/activate
```

### âœ… 3. Install Dependencies
```bash
pip install torch tokenizers
```

---

## ğŸš€ Run Instructions

### âœ… Train the Tokenizer
```bash
python scripts/train_tokenizer.py
```

### âœ… Train the Language Model
```bash
python scripts/train_lm.py
```

### âœ… Generate Text
```bash
python scripts/generate.py
```

---

## ğŸ§  What You'll Learn

- Custom tokenization using BPE
- Building transformer LMs from scratch
- Model training with PyTorch
- Sampling techniques (top-k vs top-p)
- Practical project structuring for LLMs

---

## ğŸŒ± Future Improvements

- Add Gradio app for live demo
- Train on a larger or real domain corpus (e.g., PubMed)
- Add TensorBoard logging
- Convert to HuggingFace-compatible `AutoModel`
- Add model evaluation (perplexity)

---

## ğŸ“„ License
MIT License
